{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleSentenceBgramLModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcK3ntJqajwPJDyZ/R2zH3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avishek2020/A-to-Z-Resources-for-Students/blob/master/SimpleSentenceBgramLModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGrvvZXeNfwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "207ef18d-5685-4803-a74d-6be971fb0c02"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk, re, pprint\n",
        "from difflib import SequenceMatcher, get_close_matches, Differ\n",
        "from collections import Counter\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "from time import time\n",
        "import collections\n",
        "from nltk import bigrams, trigrams\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.phrases import Phrases,Phraser\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from math import log, e\n",
        "import pandas as pd\n",
        "\n",
        "import timeit\n",
        "import inflect\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPpEjVWNNrjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3b0740a-e834-4a8c-9bb1-ef07874ea8f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Abs9THcOtrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculates the perplexity of the given text.This is simply 2 ** cross-entropy for the text.\n",
        "def perplexity(entropy):\n",
        "    #entropy = entropy(text)\n",
        "    #print(entropy)\n",
        "    return pow(2.0, entropy)\n",
        "\n",
        "# entropy\n",
        "def entropy4(labels, base=None):\n",
        "  value,counts = np.unique(labels, return_counts=True)\n",
        "  norm_counts = counts / counts.sum()\n",
        "  base = e if base is None else base\n",
        "  return -(norm_counts * np.log(norm_counts)/np.log(base)).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQOejHIlN4Rh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "74c167d2-a4b1-4219-be5b-67329137e68b"
      },
      "source": [
        "data = []   \n",
        "sentences = nltk.data.load('/content/gdrive/My Drive/ProjectTextAnalysis/parawiki_english05' , format='raw')\n",
        "for line in (sentences.decode().split('\\n')):\n",
        "    #print(line)    \n",
        "    data.append(line)\n",
        "df = pd.DataFrame(data)\n",
        "df_new =(df[0].str.split('\\t',expand=True,))\n",
        "df_new.columns = ['Complex Sentence', 'Simple Sentence','Similarity']\n",
        "df_new.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Complex Sentence</th>\n",
              "      <th>Simple Sentence</th>\n",
              "      <th>Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Following the revolution , he organized the fi...</td>\n",
              "      <td>Following the revolution he organized the firs...</td>\n",
              "      <td>0.999061161113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Other nearby towns include Haywards Heath to t...</td>\n",
              "      <td>Other nearby towns include Haywards Heath to t...</td>\n",
              "      <td>0.998915119114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The project comprised Mei Foo Interchange ( mo...</td>\n",
              "      <td>The project comprised Mei Foo Interchange ( mo...</td>\n",
              "      <td>0.998909131627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pepsi encouraged price-watching consumers to s...</td>\n",
              "      <td>Pepsi encouraged price-watching consumers to s...</td>\n",
              "      <td>0.998829368955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In Britain , musical comedian Mitch Benn has p...</td>\n",
              "      <td>In Britain , musical comedian Mitch Benn has p...</td>\n",
              "      <td>0.998813737193</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    Complex Sentence  ...      Similarity\n",
              "0  Following the revolution , he organized the fi...  ...  0.999061161113\n",
              "1  Other nearby towns include Haywards Heath to t...  ...  0.998915119114\n",
              "2  The project comprised Mei Foo Interchange ( mo...  ...  0.998909131627\n",
              "3  Pepsi encouraged price-watching consumers to s...  ...  0.998829368955\n",
              "4  In Britain , musical comedian Mitch Benn has p...  ...  0.998813737193\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztx5tatnsdGp",
        "colab_type": "text"
      },
      "source": [
        "## Simple Sentence Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBCs7tXcPGkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c912d1ef-8c8d-4d8f-db6b-11e9b345c289"
      },
      "source": [
        "df_new_ss = df_new['Simple Sentence']\n",
        "len(df_new_ss) \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "492994"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1cXzLoWOggj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e33e2bd0-32e8-4b67-ef11-a2c289749047"
      },
      "source": [
        "#split sentences into training and testing sets\n",
        "from math import floor\n",
        "def get_training_and_testing_sets(file_list):\n",
        "    split = 0.7\n",
        "    split_index = floor(len(file_list) * split)\n",
        "    training = file_list[:split_index]\n",
        "    testing = file_list[split_index:]\n",
        "    return training, testing\n",
        "\n",
        "#\n",
        "training_ss, testing_ss = get_training_and_testing_sets(df_new_ss)\n",
        "print(f\" | SIMPLE SENTENCES length of training set {len(training_ss)} | length of testing set {len(testing_ss)} | Total length = {len(training_ss)+len(testing_ss)}\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " | SIMPLE SENTENCES length of training set 345095 | length of testing set 147899 | Total length = 492994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1po5Q1ohhh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "da66996f-7844-4cf5-d302-7fe616f311f6"
      },
      "source": [
        "print(training_ss[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following the revolution he organized the first election process in Egyptian universities to choose his successor as Dean of the Faculty in March 2011 , as he left the country on 15.05.2011 in the capacity of cultural counselor at the Egyptian Embassy to Berlin and head of educational mission there .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjOesIfbO2fE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9b655ac-7ec3-4c5e-e504-db9cd1e86f6d"
      },
      "source": [
        "#df_new.shape[0] # no of col#\n",
        "training_ss.shape[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "345095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNia-BUoPTU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "84c6ef27-e93a-4ecb-a3fb-d8fde2c71237"
      },
      "source": [
        "sent_column_ss =\"Simple Sentence\"\n",
        "sent_token_ss = []\n",
        "for i in range(training_ss.shape[0]):\n",
        "    if df_new[sent_column_ss][i] is not None:\n",
        "        sentence_ss = df_new[sent_column_ss][i]            \n",
        "        #print(sentence_ss)\n",
        "        #print(i)\n",
        "        # with each sentence a list of tokens in lower is formed:\n",
        "        sentRowSS= [doc.lower() for doc in sentence_ss.split(\" \")]\n",
        "        sent_token_ss.extend(sentRowSS)\n",
        "print(\"Simple Sentence token --\",sent_token_ss[:10])\n",
        " # Train a Simple Sentence bigram model.\n",
        "\"\"\"\n",
        "min_count- ignore all words and bigrams with total collected count lower than this.\n",
        "threshold- represents a threshold for forming the phrases (higher means fewer phrases). A phrase of words `a` and `b` is accepted if\n",
        "             `(cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold`, where `N` is the total vocabulary size.\n",
        "\"\"\"          \n",
        "print(\"=== TRAIN SIMPLE SENTENCES BIGRAM MODEL ===\")\n",
        "bigram_ss = Phrases([sent_token_ss], min_count=1, threshold=1) # higher threshold fewer phrases.\n",
        "#print([phrase for phrase, score in phrases.export_phrases([sent_token])],[score for phrase, score in phrases.export_phrases([sent_token])]) \n",
        "# Export the trained model\n",
        "bigram_mod_ss = Phraser(bigram_ss)\n",
        "print(\"=== SIMPLE SENTENCES BIGRAM MODEL TRAINING DONE===\")\n",
        "# Getting all Bigram from sentence with threshold meet\n",
        "#for phrase, score in phrases.export_phrases([sent_token]):\n",
        "    #print(u'{0}   {1}'.format(phrase, score)) \n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple Sentence token -- ['following', 'the', 'revolution', 'he', 'organized', 'the', 'first', 'election', 'process', 'in']\n",
            "=== TRAIN SIMPLE SENTENCES BIGRAM MODEL ===\n",
            "=== SIMPLE SENTENCES BIGRAM MODEL TRAINING DONE===\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-z3mh-NQF58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23e215ba-f897-4908-961d-f9ac1394f49e"
      },
      "source": [
        "# Lemmatize the sentences.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs_ss = [[lemmatizer.lemmatize(token) for token in sent_token_ss]]\n",
        "\n",
        "# To get all Bigram tokens\n",
        "docs_bigm_ss = []\n",
        "trainPhrases_ss = []\n",
        "for idx in range(len(docs_ss)):\n",
        "    for token in bigram_ss[docs_ss[idx]]:\n",
        "        trainPhrases_ss.append(token)\n",
        "        if '_' in token:\n",
        "            #print(token)\n",
        "            # Token is a bigram, add to document.\n",
        "            #docs_ss[idx].append(token)\n",
        "            docs_bigm_ss.append(token)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUla1kaMSW14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b96dbcea-d20a-445b-ca15-19567a7128b6"
      },
      "source": [
        "trainPhrases_ss[:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['following',\n",
              " 'the',\n",
              " 'revolution',\n",
              " 'he_organized',\n",
              " 'the_first',\n",
              " 'election_process',\n",
              " 'in',\n",
              " 'egyptian',\n",
              " 'university',\n",
              " 'to_choose']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlV0H7wlRO-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4627423f-7de8-4562-e9cf-ba2007f38d5c"
      },
      "source": [
        "## ENTROPY of Train Model SS\n",
        "print(f\" Simple Sentences LM Entropy before training set {entropy4(sent_token_ss)}   and Perplexity {perplexity(entropy4(sent_token_ss))}\")\n",
        "print(f\" Simple Sentences LM Entropy after training set  {entropy4(trainPhrases_ss)}  and Perplexity {perplexity(entropy4(trainPhrases_ss))}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Simple Sentences LM Entropy before training set 7.03568702846942   and Perplexity 131.2057402011086\n",
            " Simple Sentences LM Entropy after training set  8.760223510629674  and Perplexity 433.6007706563789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p-JTimrBPau",
        "colab_type": "text"
      },
      "source": [
        "##### # Test Data set Useen SS Sentences 30 %[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfjbZnPf3T_b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bd863bb-bf2c-46d8-bce1-31e0f11a70ea"
      },
      "source": [
        "# Test Data set Useen Sentences 30 %\n",
        "testing_ss.shape[0]\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147899"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMH5lx264QA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a203801-1eb9-4b96-8341-4fc2d03fd4cb"
      },
      "source": [
        "sent_column_ss =\"Simple Sentence\"\n",
        "sent_token_ss_tst = []\n",
        "for i in list(testing_ss.index.values):\n",
        "    if df_new[sent_column_ss][i] is not None:\n",
        "        sentence_ss_tst = df_new[sent_column_ss][i]            \n",
        "        sentRowSS_tst= [doc.lower() for doc in sentence_ss_tst.split(\" \")]\n",
        "        sent_token_ss_tst.extend(sentRowSS_tst)\n",
        "print(\"Simple Sentence test set token --\",sent_token_ss_tst[:10])\n",
        "#        \n",
        "# Lemmatize the sentences.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs_ss_tst = [[lemmatizer.lemmatize(token) for token in sent_token_ss_tst]]\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simple Sentence test set token -- ['lleida', '(', ',', ',', ')', 'is', 'a', 'city', 'in', 'western']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oth-kTaC6cNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29f532d2-3b2f-4dbd-c62f-bd4879d5c0ee"
      },
      "source": [
        "print(docs_ss_tst[0][:10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lleida', '(', ',', ',', ')', 'is', 'a', 'city', 'in', 'western']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzWFdUAF6lUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f96904d7-b551-40ee-ade8-9537b6d03431"
      },
      "source": [
        "# # Passing unseen simple sentences test set to Train Bigram model\n",
        "docs_bigm_ss_tst = []\n",
        "trainPhrases_ss_tst = []\n",
        "for idx in range(len(docs_ss_tst)):\n",
        "    for token in bigram_ss[docs_ss_tst[idx]]:\n",
        "        trainPhrases_ss_tst.append(token)\n",
        "        if '_' in token:\n",
        "            #print(token)\n",
        "            # Token is a bigram, add to document.\n",
        "            #docs_ss[idx].append(token)\n",
        "            docs_bigm_ss_tst.append(token)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "756Fmvum7WGm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bad41a1f-0a9e-4e82-88ba-bb0ef491c0dd"
      },
      "source": [
        "## ENTROPY of Test set SS\n",
        "print(f\" Test Data Simple Sentences LM Entropy before prediction(feeding to Model) {entropy4(sent_token_ss_tst)}   and Perplexity {perplexity(entropy4(sent_token_ss_tst))}\")\n",
        "print(f\" Test Data Simple Sentences LM Entropy after  prediction(feeding  to Model)  {entropy4(trainPhrases_ss_tst)}  and Perplexity {perplexity(entropy4(trainPhrases_ss_tst))}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Test Data Simple Sentences LM Entropy before prediction(feeding to Model) 6.7998721907021675   and Perplexity 111.42060084071281\n",
            " Test Data Simple Sentences LM Entropy after  prediction(feeding  to Model)  8.386915400646417  and Perplexity 334.74423531371644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bst0ymXklxHc",
        "colab_type": "text"
      },
      "source": [
        "## Complex Sentences part\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LtZgZJQgXmL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20855277-c430-41f5-a5f1-3f960523a93e"
      },
      "source": [
        "df_new_cs = df_new['Complex Sentence']\n",
        "len(df_new_cs)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "492994"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGnDQCM0Yvtk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad2ee81f-3498-41d8-e263-1b5edb60fc26"
      },
      "source": [
        "#\n",
        "training_cs, testing_cs = get_training_and_testing_sets(df_new_cs)\n",
        "print(f\" | COMPLEX SENTENCES length of training set {len(training_cs)} | length of testing set {len(testing_cs)} | Total length = {len(training_cs)+len(testing_cs)}\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " | COMPLEX SENTENCES length of training set 345095 | length of testing set 147899 | Total length = 492994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCM1M6iCnABm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "48341423-32de-45d6-aeb7-c8da38985fa6"
      },
      "source": [
        "print(training_cs[0])\n",
        "print(training_ss[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following the revolution , he organized the first election process in Egyptian universities to choose his successor as Dean of the Faculty in March 2011 , as he left the country on 15 May 2011 in the capacity of cultural counselor at the Egyptian Embassy to Berlin and head of the educational mission there .\n",
            "Following the revolution he organized the first election process in Egyptian universities to choose his successor as Dean of the Faculty in March 2011 , as he left the country on 15.05.2011 in the capacity of cultural counselor at the Egyptian Embassy to Berlin and head of educational mission there .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb_kAyfIrdMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbc89c2d-bae3-4f26-b107-0a8fc185fc2b"
      },
      "source": [
        "#df_new.shape[0] # no of col#\n",
        "training_cs.shape[0]"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "345095"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UiSZ5YunAWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4ab7c1e9-08ab-4142-8fdb-3f3124d5159f"
      },
      "source": [
        "sent_column_cs =\"Complex Sentence\"\n",
        "sent_token_cs = []\n",
        "for i in range(training_cs.shape[0]):\n",
        "    if df_new[sent_column_cs][i] is not None:\n",
        "        sentence_cs = df_new[sent_column_cs][i]            \n",
        "        #print(sentence_ss)\n",
        "        #print(i)\n",
        "        # with each sentence a list of tokens in lower is formed:\n",
        "        sentRowCS= [doc.lower() for doc in sentence_cs.split(\" \")]\n",
        "        sent_token_cs.extend(sentRowCS)\n",
        "print(\"Complex Sentence token --\",sent_token_cs[:10])\n",
        " # Train a Complex Sentence bigram model.\n",
        "\"\"\"\n",
        "min_count- ignore all words and bigrams with total collected count lower than this.\n",
        "threshold- represents a threshold for forming the phrases (higher means fewer phrases). A phrase of words `a` and `b` is accepted if\n",
        "             `(cnt(a, b) - min_count) * N / (cnt(a) * cnt(b)) > threshold`, where `N` is the total vocabulary size.\n",
        "\"\"\"          \n",
        "print(\"=== TRAIN SIMPLE SENTENCES BIGRAM MODEL ===\")\n",
        "bigram_cs = Phrases([sent_token_cs], min_count=1, threshold=1) # higher threshold fewer phrases.\n",
        "#print([phrase for phrase, score in phrases.export_phrases([sent_token])],[score for phrase, score in phrases.export_phrases([sent_token])]) \n",
        "# Export the trained model\n",
        "bigram_mod_cs = Phraser(bigram_cs)\n",
        "print(\"=== COMPLEX SENTENCES BIGRAM MODEL TRAINING DONE===\")\n",
        "# Getting all Bigram from sentence with threshold meet\n",
        "#for phrase, score in phrases.export_phrases([sent_token]):\n",
        "    #print(u'{0}   {1}'.format(phrase, score)) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complex Sentence token -- ['following', 'the', 'revolution', ',', 'he', 'organized', 'the', 'first', 'election', 'process']\n",
            "=== TRAIN SIMPLE SENTENCES BIGRAM MODEL ===\n",
            "=== COMPLEX SENTENCES BIGRAM MODEL TRAINING DONE===\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz5H1PKOrxWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e9596b55-e3d4-4788-c9a3-849944aa274f"
      },
      "source": [
        "# Lemmatize the sentences.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs_cs = [[lemmatizer.lemmatize(token) for token in sent_token_cs]]\n",
        "\n",
        "# To get all Bigram tokens\n",
        "docs_bigm_cs = []\n",
        "trainPhrases_cs = []\n",
        "for idx in range(len(docs_cs)):\n",
        "    for token in bigram_cs[docs_cs[idx]]:\n",
        "        trainPhrases_cs.append(token)\n",
        "        if '_' in token:\n",
        "            #print(token)\n",
        "            # Token is a bigram, add to document.\n",
        "            #docs_ss[idx].append(token)\n",
        "            docs_bigm_cs.append(token)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjPRg6ddr1En",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4c35b35f-d6e2-4971-8f2b-d13c41505e7e"
      },
      "source": [
        "trainPhrases_cs[:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['following',\n",
              " 'the',\n",
              " 'revolution',\n",
              " ',',\n",
              " 'he_organized',\n",
              " 'the_first',\n",
              " 'election_process',\n",
              " 'in',\n",
              " 'egyptian',\n",
              " 'university']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvL5AgUevOR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7980e2c0-8de9-4886-baea-f2e6889dc172"
      },
      "source": [
        "## ENTROPY of Train Model SS\n",
        "print(f\" Complex Sentences LM Entropy before training set {entropy4(sent_token_cs)}   and Perplexity {perplexity(entropy4(sent_token_cs))}\")\n",
        "print(f\" Complex Sentences LM Entropy after training set  {entropy4(trainPhrases_cs)}  and Perplexity {perplexity(entropy4(trainPhrases_cs))}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Complex Sentences LM Entropy before training set 7.172198915138625   and Perplexity 144.2271455503961\n",
            " Complex Sentences LM Entropy after training set  8.78316712286554  and Perplexity 440.5515777393934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ_B6KZDDETo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBopcgFmBi9Y",
        "colab_type": "text"
      },
      "source": [
        " ##### Test Data set Useen CS Sentences 30 % "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBXLg_n0Ephp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f544d5d-b39c-4415-c6e4-27f0787f3ee3"
      },
      "source": [
        "# Test Data set Useen Sentences 30 %\n",
        "testing_cs.shape[0]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147899"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb5g9KSoDF9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1098a16d-d7cc-4dc9-d876-eb53e85da6b9"
      },
      "source": [
        "sent_column_cs =\"Complex Sentence\"\n",
        "sent_token_cs_tst = []\n",
        "for i in list(testing_cs.index.values):\n",
        "    if df_new[sent_column_cs][i] is not None:\n",
        "        sentence_cs_tst = df_new[sent_column_cs][i]            \n",
        "        sentRowCS_tst= [doc.lower() for doc in sentence_cs_tst.split(\" \")]\n",
        "        sent_token_cs_tst.extend(sentRowCS_tst)\n",
        "print(\"Complex Sentence test set token --\",sent_token_cs_tst[:10])\n",
        "#        \n",
        "# Lemmatize the sentences.\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "docs_cs_tst = [[lemmatizer.lemmatize(token) for token in sent_token_cs_tst]]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Complex Sentence test set token -- ['it', 'is', 'the', 'capital', 'city', 'of', 'the', 'province', 'of', 'lleida']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq1p8rGrEb7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff01fb42-4eaf-4957-e6e4-1604444bee1f"
      },
      "source": [
        "print(docs_cs_tst[0][:10])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['it', 'is', 'the', 'capital', 'city', 'of', 'the', 'province', 'of', 'lleida']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPudlDuLDGXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6c948d9d-f980-4669-fb63-ed1d9f1dbc0c"
      },
      "source": [
        "# # Passing unseen Complex sentences test set to Train Bigram model\n",
        "docs_bigm_cs_tst = []\n",
        "trainPhrases_cs_tst = []\n",
        "for idx in range(len(docs_cs_tst)):\n",
        "    for token in bigram_cs[docs_cs_tst[idx]]:\n",
        "        trainPhrases_cs_tst.append(token)\n",
        "        if '_' in token:\n",
        "            #print(token)\n",
        "            # Token is a bigram, add to document.\n",
        "            #docs_ss[idx].append(token)\n",
        "            docs_bigm_cs_tst.append(token)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znXMCntEDG2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9dd68799-df88-4960-a919-065501cf3275"
      },
      "source": [
        "## ENTROPY of Test set CS\n",
        "print(f\" Test Data Complex Sentences LM Entropy before prediction(feeding to Model) {entropy4(sent_token_cs_tst)}   and Perplexity {perplexity(entropy4(sent_token_cs_tst))}\")\n",
        "print(f\" Test Data Complex Sentences LM Entropy after  prediction(feeding  to Model)  {entropy4(trainPhrases_cs_tst)}  and Perplexity {perplexity(entropy4(trainPhrases_cs_tst))}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Test Data Complex Sentences LM Entropy before prediction(feeding to Model) 7.092316799411957   and Perplexity 136.45834362623935\n",
            " Test Data Complex Sentences LM Entropy after  prediction(feeding  to Model)  8.503500926215906  and Perplexity 362.91828251928234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoZ7xW3_HmVP",
        "colab_type": "text"
      },
      "source": [
        "##### Simple Sentence Trained Model Predicting Unseen test Complex sentences set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH0y7zhzH0ag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "78ee43af-482f-484c-a33a-2e068aad4806"
      },
      "source": [
        "docs_bigm_ssUcs = []\n",
        "trainPhrases_ssUcs = []\n",
        "for idx in range(len(docs_cs_tst)):\n",
        "    for token in bigram_ss[docs_cs_tst[idx]]:\n",
        "        trainPhrases_ssUcs.append(token)\n",
        "        if '_' in token:\n",
        "            #print(token)\n",
        "            # Token is a bigram, add to document.\n",
        "            #docs_ss[idx].append(token)\n",
        "            docs_bigm_ssUcs.append(token)\n",
        "print(f\" Simple Sentence Trained Model Predicting Unseen test Complex sentences set LM Entropy  {entropy4(trainPhrases_ssUcs)}  and Perplexity {perplexity(entropy4(trainPhrases_ssUcs))}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Simple Sentence Trained Model Predicting Unseen test Complex sentences set LM Entropy  8.55018770127335  and Perplexity 374.85470531424124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw0P-HHlHy47",
        "colab_type": "text"
      },
      "source": [
        "##### Complex Sentence Trained Model Predicting Unseen test Simple sentence set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nNRmUErLwF9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "049176f7-773b-47b5-8293-2f3efb6b53b5"
      },
      "source": [
        "docs_bigm_csUss = []\n",
        "trainPhrases_csUss = []\n",
        "for idx in range(len(docs_ss_tst)):\n",
        "    for token in bigram_cs[docs_ss_tst[idx]]:\n",
        "        trainPhrases_csUss.append(token)\n",
        "        if '_' in token:\n",
        "            #print(token)\n",
        "            # Token is a bigram, add to document.\n",
        "            #docs_ss[idx].append(token)\n",
        "            docs_bigm_csUss.append(token)\n",
        "print(f\" Complex Sentence Trained Model Predicting Unseen test Simple sentences set LM Entropy  {entropy4(trainPhrases_csUss)}  and Perplexity {perplexity(entropy4(trainPhrases_csUss))}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Complex Sentence Trained Model Predicting Unseen test Simple sentences set LM Entropy  8.225147324874545  and Perplexity 299.23753155630885\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}