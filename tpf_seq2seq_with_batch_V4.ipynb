{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tpf_seq2seq_with_batch_V3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avishek2020/A-to-Z-Resources-for-Students/blob/master/tpf_seq2seq_with_batch_V4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRdvyHnM4Gsn"
      },
      "source": [
        "\"\"\"## Changes to be done\n",
        "\n",
        "1. Plotting on each iteration\n",
        "2. Early stop\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkfzcFMBOpPx"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "#\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = { 0:\"PAD\" , 1: \"SOS\", 3: \"EOS\"}\n",
        "        self.n_words = 3  # Count SOS and EOS\n",
        "        self.max_len = 0\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        word_len = len(sentence.split(\" \"))\n",
        "        if(word_len > self.max_len):\n",
        "            self.max_len = word_len\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VtDRrkbLUGp"
      },
      "source": [
        "### To Get information of No of core of GPU use nvidia-smi-L"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmoH43LdLfYR",
        "outputId": "fc042126-98f7-475a-d640-837958f93308"
      },
      "source": [
        "# !nvidia-smi -L\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Oct 31 21:34:05 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 456.71       Driver Version: 456.71       CUDA Version: 11.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  GeForce RTX 2060   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   62C    P0    22W /  N/A |    611MiB /  6144MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      8512    C+G   ...cast\\NVIDIA Broadcast.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K27eWQrNMkP8"
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1ktoFWaMqv3"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "  # Read the file and split into lines\n",
        "    lines = open('./data/simplecomplexPWKP', encoding='utf-8').read().strip().split('\\n')\n",
        "    \n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    print(len(pairs))\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFwaoqwjMtM0",
        "outputId": "a44421da-fa05-451b-a21f-ee1346724968"
      },
      "source": [
        "MAX_LENGTH = 42\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# Prepare Data\n",
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('Simple', 'Complex', False)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "108016\n",
            "Read 108016 sentence pairs\n",
            "Trimmed to 104476 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "Simple 68016\n",
            "Complex 81469\n",
            "['from the th century to the sixteenth century the devotion was propagated but it did not seem to have developed in itself .', 'from the thirteenth to the sixteenth centuries the devotion was propagated but it did not seem to have developed in itself .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSbu_FN04GtD",
        "outputId": "a0194a0a-47c5-4be3-b420-a25e922eb128"
      },
      "source": [
        "len(pairs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "104476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ci_6gVLRJS_"
      },
      "source": [
        "# Encoder & Decoder\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size , batch_size=64):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size , batch_first=True)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        #view(self.batch_size, 1, -1)\n",
        "        embedded = self.embedding(input).view(input.shape[0], 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros( 1 , batch_size, self.hidden_size, device=device)\n",
        "    \n",
        "#simple Decorder additional------\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, batch_size =64):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first =True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH , batch_size =64):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = 64\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size , batch_first=True)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input_tensor, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_tensor).view(input_tensor.shape[0], 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[: , 0 , :], hidden[0 , : ,:]), 1)), dim=1)\n",
        "        \n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
        "                                 encoder_outputs)\n",
        "\n",
        "        output = torch.cat((embedded[: ,0,: ], attn_applied[: , 0 , : ]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = F.log_softmax(self.out(output[:,0,:]), dim=-1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size ,  self.hidden_size, device=device)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsXwOHdKRJXZ"
      },
      "source": [
        "#\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    out_list = [ 0 for i in range(lang.max_len)  ]\n",
        "    for idx , word in enumerate(sentence.split(' ')):\n",
        "        out_list[idx] = lang.word2index[word]\n",
        "    return out_list\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(sentence, type=\"input\"):\n",
        "    if(type==\"input\"):\n",
        "        out_tensor = tensorFromSentence(input_lang, sentence)\n",
        "    else:\n",
        "        out_tensor = tensorFromSentence(output_lang, sentence)\n",
        "    return out_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDtwlzJmSSeF"
      },
      "source": [
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KylrAK5ltmr"
      },
      "source": [
        "### Referernce link \n",
        "https://github.com/pengyuchen/PyTorch-Batch-Seq2seq/blob/master/seq2seq_translation_tutorial.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dVSVt6g4GtW"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    \n",
        "    batch_size = input_tensor.size(0)\n",
        "    \n",
        "    encoder_hidden = encoder.initHidden(batch_size)\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(1)\n",
        "    target_length = target_tensor.size(1)\n",
        "\n",
        "    encoder_outputs = torch.zeros(batch_size ,max_length , encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[:, ei ,: ] , encoder_hidden)\n",
        "        encoder_outputs[: , ei , : ] = encoder_output[: , 0 , :]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]*batch_size], device=device)\n",
        "    decoder_input = decoder_input.view(batch_size , -1)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input , decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[:,di,:].view(batch_size))\n",
        "            decoder_input = target_tensor[:,di,:]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            loss += criterion(decoder_output, target_tensor[:,di,:].view(batch_size))\n",
        "            #if decoder_input.item() == EOS_token:\n",
        "            #    break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "\n",
        "# def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    \n",
        "#     batch_size = input_tensor.size(0)\n",
        "\n",
        "#     encoder_hidden = encoder.initHidden(batch_size)\n",
        "#     encoder_optimizer.zero_grad()\n",
        "#     decoder_optimizer.zero_grad()\n",
        "\n",
        "#     input_length = input_tensor.size(1)\n",
        "#     target_length = target_tensor.size(1)\n",
        "\n",
        "#     encoder_outputs = torch.zeros(batch_size ,max_length , encoder.hidden_size, device=device)\n",
        "\n",
        "#     loss = 0\n",
        "\n",
        "#     for ei in range(input_length):\n",
        "#         encoder_output, encoder_hidden = encoder(input_tensor[:, ei ,: ] , encoder_hidden)\n",
        "#         encoder_outputs[: , ei , : ] = encoder_output[: , 0 , :]\n",
        "\n",
        "#     decoder_input = torch.tensor([[SOS_token]*batch_size], device=device)\n",
        "#     decoder_input = decoder_input.view(batch_size , -1)\n",
        "#     decoder_hidden = encoder_hidden\n",
        "\n",
        "#     use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "#     if use_teacher_forcing:\n",
        "#             # Teacher forcing: Feed the target as the next input\n",
        "#             for di in range(target_length):\n",
        "#                 decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "#                     decoder_input , decoder_hidden, encoder_outputs)\n",
        "#                 loss += criterion(decoder_output, target_tensor[:,di,:].view(batch_size))\n",
        "#                 decoder_input = target_tensor[:,di,:]  # Teacher forcing\n",
        "\n",
        "#     else:\n",
        "#             # Without teacher forcing: use its own predictions as the next input\n",
        "#             for di in range(target_length):\n",
        "#                 decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "#                     decoder_input, decoder_hidden, encoder_outputs)\n",
        "#                 topv, topi = decoder_output.topk(1)\n",
        "#                 decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "#                 loss += criterion(decoder_output, target_tensor[:,di,:].view(batch_size))\n",
        "#                 #if decoder_input.item() == EOS_token:\n",
        "#                 #    break\n",
        "\n",
        "#         loss.backward()\n",
        "\n",
        "#         encoder_optimizer.step()\n",
        "#         decoder_optimizer.step()\n",
        "        \n",
        "#         avg_Loss = loss.item() / target_length\n",
        "#         train_losses.append(avg_Loss)\n",
        "\n",
        "#     return train_losses #loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMt787c-4Gtb"
      },
      "source": [
        "def validate(input_tensor, target_tensor, encoder, decoder, criterion, max_length=MAX_LENGTH):\n",
        "    \n",
        "    batch_size = input_tensor.size(0)\n",
        "    \n",
        "    encoder_hidden = encoder.initHidden(batch_size)\n",
        "    \n",
        "    input_length = input_tensor.size(1)\n",
        "    target_length = target_tensor.size(1)\n",
        "\n",
        "    encoder_outputs = torch.zeros(batch_size ,max_length , encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[:, ei ,: ] , encoder_hidden)\n",
        "        encoder_outputs[: , ei , : ] = encoder_output[: , 0 , :]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]*batch_size], device=device)\n",
        "    decoder_input = decoder_input.view(batch_size , -1)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input , decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[:,di,:].view(batch_size))\n",
        "            decoder_input = target_tensor[:,di,:]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            loss += criterion(decoder_output, target_tensor[:,di,:].view(batch_size))\n",
        "            #if decoder_input.item() == EOS_token:\n",
        "            #    break\n",
        "\n",
        "    #\n",
        "    return loss.item() / target_length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMWwpvYM4Gth"
      },
      "source": [
        "#### Early stopping Logic\n",
        " 1) https://github.com/Bjarten/early-stopping-pytorch/blob/master/MNIST_Early_Stopping_example.ipynb\n",
        " \n",
        " 2) https://charon.me/posts/pytorch/pytorch_seq2seq_1/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzOjGNOs4Gti"
      },
      "source": [
        "# 80 10 10 , patince\n",
        "# !pip install pytorchtools\n",
        "# import EarlyStopping\n",
        "# from pytorchtools import EarlyStopping\n",
        "# https://raw.githubusercontent.com/Bjarten/early-stopping-pytorch/master/pytorchtools.py\n",
        "# patience: Number of epochs with no improvement after which training will be stopped.\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model_encoder , model_decoder):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model_encoder , model_decoder)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model_encoder , model_decoder)\n",
        "            #self.save_checkpoint(val_loss, model_encoder , \"encoder_model.pth\")\n",
        "            #self.save_checkpoint(val_loss, model_decoder , \"decoder_model.pth\")\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model_encoder , model_decoder):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        #torch.save(model.state_dict(), model_path)\n",
        "        torch.save(model_encoder.state_dict(),  './save_model/encoder.dict')\n",
        "        torch.save(model_decoder.state_dict(),  './save_model/decoder.dict')\n",
        "        #Saving model every 2 epoch wise:\n",
        "        #torch.save(encoder1.state_dict(), './save_model/epoch_'+str(epoch)+'_encoder.dict')\n",
        "        #torch.save(attn_decoder1.state_dict(), './save_model/epoch_'+str(epoch)+'_decoder.dict')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k-hbKS4R_Ho"
      },
      "source": [
        "# Plotting \n",
        "\n",
        "def showPlot(points, iters):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    #this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    dest = './figures/fig_1_'+str(iters)+'.png'\n",
        "    plt.savefig(dest)\n",
        "    plt.close('all') \n",
        "\n",
        "\n",
        "def showIterWiseLossPlot(plot_train_iter_losses, plot_valid_iter_losses):\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(range(1,len(plot_train_iter_losses)+1), plot_train_iter_losses, label='Training Loss')\n",
        "    plt.plot(range(1,len(plot_valid_iter_losses)+1), plot_valid_iter_losses,label='Validation Loss')\n",
        "\n",
        "    # find position of lowest validation loss\n",
        "    minposs = plot_valid_iter_losses.index(min(plot_valid_iter_losses))+1 \n",
        "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "    plt.xlabel(f\"No of iter {n_iters}\")\n",
        "    plt.ylabel('loss')\n",
        "    #plt.ylim(0, 0.5) # consistent scale\n",
        "    plt.xlim(0, len(plot_train_iter_losses)+1) # consistent scale n_iters   \n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig('./figures/iterwiseLoss_plot.png', bbox_inches='tight') \n",
        "    \n",
        "    \n",
        "def showLossEarlyPlot(avg_train_losses, avg_valid_losses):\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(range(1,len(avg_train_losses)+1), avg_train_losses, label='Training Loss')\n",
        "    plt.plot(range(1,len(avg_valid_losses)+1), avg_valid_losses,label='Validation Loss')\n",
        "\n",
        "    # find position of lowest validation loss\n",
        "    minposs = avg_valid_losses.index(min(avg_valid_losses))+1 \n",
        "    plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "    plt.xlabel(f\"iters - No of batch {len(train_loader)} x iter {n_iters}\")\n",
        "    plt.ylabel('loss')\n",
        "    #plt.ylim(0, 0.5) # consistent scale\n",
        "    plt.xlim(0, len(avg_train_losses)+1) # consistent scale n_iters   \n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    fig.savefig('./figures/loss_Early_plot.png', bbox_inches='tight')\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "def trainIters(encoder, decoder, n_iters, train_loader, test_loader, valid_loader, print_every=10, plot_every=10, learning_rate=0.001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    plot_lossBatch_total = 0\n",
        "    plot_batch_losses = []\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    # to track the average training loss per epoch as the model trains    \n",
        "    avg_train_losses = []\n",
        "    # to track the average validation loss per epoch as the model trains    \n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    plot_train_iter_loss_avg = 0\n",
        "    plot_train_iter_losses = []\n",
        "    plot_valid_iter_loss_avg = 0\n",
        "    plot_valid_iter_losses = []\n",
        "    plot_valid_loss_total =0\n",
        "    \n",
        "    #patience = 0\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    \n",
        "    best_valid_loss = float('inf') # Inf is infinity, it's a \"bigger than all the other numbers\" number. \n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    # training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
        "\n",
        "    # data loader\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1): # 50 plotting should be done here\n",
        "        cnt = 0\n",
        "        \n",
        "         ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        \n",
        "        for input_tensor, target_tensor in train_loader: # 1469\n",
        "            \n",
        "            \n",
        "            input_tensor  = [tensorsFromPair(i_sentence , \"input\") for i_sentence in  input_tensor ]\n",
        "            \n",
        "            target_tensor = [tensorsFromPair(i_sentence, \"target\") for i_sentence in  target_tensor ]\n",
        "            \n",
        "            input_tensor  = torch.stack(input_tensor , axis =0)\n",
        "            target_tensor = torch.stack(target_tensor , axis=0)\n",
        "        \n",
        "        \n",
        "            train_loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            \n",
        "            print_loss_total += train_loss\n",
        "            plot_loss_total += train_loss \n",
        "            \n",
        "            #avg_train_loss += train_loss\n",
        "            avg_train_losses.append(train_loss)\n",
        "            \n",
        "        if iter :\n",
        "            plot_train_iter_loss_avg = plot_loss_total / len(train_loader)\n",
        "            plot_train_iter_losses.append(plot_train_iter_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "            \n",
        "            \n",
        "        ###################\n",
        "        # validate the model #\n",
        "        ###################\n",
        "        \n",
        "        for input_tensor, target_tensor in valid_loader: # 1469\n",
        "            \n",
        "            \n",
        "            input_tensor  = [tensorsFromPair(i_sentence , \"input\") for i_sentence in  input_tensor ]\n",
        "            \n",
        "            target_tensor = [tensorsFromPair(i_sentence, \"target\") for i_sentence in  target_tensor ]\n",
        "            \n",
        "            input_tensor  = torch.stack(input_tensor , axis =0)\n",
        "            target_tensor = torch.stack(target_tensor , axis=0)\n",
        "        \n",
        "        \n",
        "            valid_loss = validate(input_tensor, target_tensor, encoder, decoder, criterion)\n",
        "            \n",
        "            avg_valid_losses.append(valid_loss)\n",
        "            #avg_valid_loss += valid_loss\n",
        "            plot_valid_loss_total+= valid_loss\n",
        "            \n",
        "        if iter :\n",
        "            plot_valid_iter_loss_avg = plot_valid_loss_total / len(valid_loader)\n",
        "            plot_valid_iter_losses.append(plot_valid_iter_loss_avg)\n",
        "            plot_valid_loss_total = 0            \n",
        "             \n",
        "\n",
        "            ###################\n",
        "        \n",
        "            #print(f\" Valid Loss {valid_loss} Int valid_loss {(valid_loss)}\")\n",
        "        \n",
        "            ############################################################\n",
        "            # calculate average loss over an epoch{in this case iter}  #\n",
        "            ############################################################\n",
        "#             avg_train_losses.append(train_loss)\n",
        "#             avg_valid_losses.append(valid_loss)\n",
        "\n",
        "            # early_stopping needs the validation loss to check if it has decresed, \n",
        "            # and if it has, it will make a checkpoint of the current model\n",
        "            early_stopping(valid_loss, encoder , decoder)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "#         if valid_loss < best_valid_loss:\n",
        "#             best_valid_loss = valid_loss\n",
        "#             torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "        #\n",
        "         \n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            #print(f'Time: {timeSince(start, iter / n_iters)} | Progress: {iter, iter / n_iters * 100}| Loss: {print_loss_avg}')\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        #if iter % plot_every == 0:\n",
        "            #plot_loss_avg = plot_loss_total / plot_every\n",
        "            #plot_losses.append(plot_loss_avg)\n",
        "            #plot_loss_total = 0\n",
        "            \n",
        "        # iter wise plotting\n",
        "        #showPlot(plot_losses, iter)\n",
        "        # new \n",
        "       # iterwise plot\n",
        "        if iter:\n",
        "            print(f\" iter {iter}\")\n",
        "        \n",
        "        #avg_train_losses.append(avg_train_loss)\n",
        "        #avg_valid_losses.append(avg_valid_loss)\n",
        "        \n",
        "    showIterWiseLossPlot(plot_train_iter_losses, plot_valid_iter_losses)\n",
        "    # Visualizing the Loss and the Early Stopping Checkpoint\n",
        "    print(f\" avg_train_losses {avg_train_losses} Int avg_valid_losses {(avg_valid_losses)}\")\n",
        "    showLossEarlyPlot(avg_train_losses, avg_valid_losses)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxYdJXAv4Gts"
      },
      "source": [
        "def train_test_val(X, train_ratio, test_ratio, val_ratio):    \n",
        "\n",
        "    ind_train = int(round(len(X)*train_ratio))\n",
        "    ind_test = int(round(len(X)*(train_ratio+test_ratio)))\n",
        "\n",
        "    X_train = X[:ind_train]\n",
        "    X_test  = X[ind_train:ind_test]\n",
        "    X_val   = X[ind_test:]\n",
        "    \n",
        "    return X_train, X_test, X_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0maTefccX-i"
      },
      "source": [
        "### Load and Batch the Data\n",
        "#### Total pairs length = 104476\n",
        "#### Batch Size = 64\n",
        "#### 90 % Training 10 % Test\n",
        "##### -size of Training Pair : 94028    size of Test pair : 10448\n",
        "\n",
        "###### -Total No of Training Batch : 1469.1875    Total No of Test Batch : 163.25\n",
        "\n",
        "###### -n_iters = 10 , then 10 * 1469 = 14690"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEtS8cop4Gtw"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "def create_datasets(batch_size):\n",
        "    #\n",
        "    # put ratio as you wish\n",
        "    train_pairs, test_pairs, val_pairs = train_test_val(pairs, 0.8, 0.1, 0.1) \n",
        "    \n",
        "    print(f\" Size of Train Pairs {len(train_pairs)} \\\n",
        "             Size of Validate Pairs {len(val_pairs)} \\\n",
        "             Size of Test Pairs {len(test_pairs)}\")\n",
        "    \n",
        "    # load training data in batches\n",
        "    train_loader = torch.utils.data.DataLoader(train_pairs, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False, \n",
        "                                           pin_memory=True , \n",
        "                                           drop_last=True)\n",
        "    \n",
        "    # load validation data in batches\n",
        "    valid_loader = torch.utils.data.DataLoader(val_pairs,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False, \n",
        "                                           pin_memory=True , \n",
        "                                           drop_last=True)\n",
        "    \n",
        "    # load test data in batches\n",
        "    test_loader = torch.utils.data.DataLoader(test_pairs, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False, \n",
        "                                          pin_memory=True , \n",
        "                                          drop_last=True)\n",
        "    \n",
        "    \n",
        " \n",
        "    \n",
        "    return train_loader, test_loader, valid_loader \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Jm5u6rkFaY"
      },
      "source": [
        "75000 X 1350"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hazRoytfkFaY"
      },
      "source": [
        "patience = 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87_732NmS_1g",
        "outputId": "3cab7746-8f17-41a9-e97d-0254fb720b13"
      },
      "source": [
        "# Define hyperparameters\n",
        "#patience = 20\n",
        "batch_size = 64\n",
        "hidden_size = 256\n",
        "n_iters  = 75000 #len(pairs) # Total no of training Pairs batch sie (len(features_train) / batch_size)\n",
        "\n",
        "train_loader, test_loader, valid_loader = create_datasets(batch_size)\n",
        "\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, n_iters, train_loader, test_loader, valid_loader, print_every=5, plot_every=5, learning_rate=0.001)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Size of Train Pairs 83581              Size of Validate Pairs 10448              Size of Test Pairs 10447\n",
            "Validation loss decreased (inf --> 4.004864).  Saving model ...\n",
            " iter 1\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 2\n",
            "Validation loss decreased (4.004864 --> 3.760181).  Saving model ...\n",
            " iter 3\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 4\n",
            "EarlyStopping counter: 2 out of 20\n",
            "40m 15s (- 603787m 58s) (5 0%) 5510.8446\n",
            " iter 5\n",
            "Validation loss decreased (3.760181 --> 3.611762).  Saving model ...\n",
            " iter 6\n",
            "Validation loss decreased (3.611762 --> 3.597796).  Saving model ...\n",
            " iter 7\n",
            "Validation loss decreased (3.597796 --> 3.567402).  Saving model ...\n",
            " iter 8\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 9\n",
            "EarlyStopping counter: 2 out of 20\n",
            "80m 32s (- 603997m 12s) (10 0%) 5115.7970\n",
            " iter 10\n",
            "Validation loss decreased (3.567402 --> 3.515219).  Saving model ...\n",
            " iter 11\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 12\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 13\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 14\n",
            "EarlyStopping counter: 4 out of 20\n",
            "120m 52s (- 604239m 54s) (15 0%) 5024.4478\n",
            " iter 15\n",
            "Validation loss decreased (3.515219 --> 3.461916).  Saving model ...\n",
            " iter 16\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 17\n",
            "Validation loss decreased (3.461916 --> 3.418260).  Saving model ...\n",
            " iter 18\n",
            "Validation loss decreased (3.418260 --> 3.405191).  Saving model ...\n",
            " iter 19\n",
            "EarlyStopping counter: 1 out of 20\n",
            "161m 7s (- 604052m 20s) (20 0%) 4930.1414\n",
            " iter 20\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 21\n",
            "Validation loss decreased (3.405191 --> 3.382294).  Saving model ...\n",
            " iter 22\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 23\n",
            "Validation loss decreased (3.382294 --> 3.370835).  Saving model ...\n",
            " iter 24\n",
            "Validation loss decreased (3.370835 --> 3.349774).  Saving model ...\n",
            "201m 22s (- 603926m 4s) (25 0%) 4870.7363\n",
            " iter 25\n",
            "Validation loss decreased (3.349774 --> 3.346221).  Saving model ...\n",
            " iter 26\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 27\n",
            "Validation loss decreased (3.346221 --> 3.343391).  Saving model ...\n",
            " iter 28\n",
            "Validation loss decreased (3.343391 --> 3.319884).  Saving model ...\n",
            " iter 29\n",
            "Validation loss decreased (3.319884 --> 3.301154).  Saving model ...\n",
            "241m 40s (- 603963m 45s) (30 0%) 4831.6023\n",
            " iter 30\n",
            "Validation loss decreased (3.301154 --> 3.292500).  Saving model ...\n",
            " iter 31\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 32\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 33\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 34\n",
            "EarlyStopping counter: 4 out of 20\n",
            "281m 52s (- 603721m 54s) (35 0%) 4779.0646\n",
            " iter 35\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 36\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 37\n",
            "EarlyStopping counter: 7 out of 20\n",
            " iter 38\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 39\n",
            "Validation loss decreased (3.292500 --> 3.227337).  Saving model ...\n",
            "322m 7s (- 603670m 3s) (40 0%) 4747.4681\n",
            " iter 40\n",
            "Validation loss decreased (3.227337 --> 3.224245).  Saving model ...\n",
            " iter 41\n",
            "Validation loss decreased (3.224245 --> 3.217443).  Saving model ...\n",
            " iter 42\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 43\n",
            "Validation loss decreased (3.217443 --> 3.203599).  Saving model ...\n",
            " iter 44\n",
            "EarlyStopping counter: 1 out of 20\n",
            "362m 22s (- 603599m 48s) (45 0%) 4710.6970\n",
            " iter 45\n",
            "Validation loss decreased (3.203599 --> 3.200846).  Saving model ...\n",
            " iter 46\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 47\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 48\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 49\n",
            "Validation loss decreased (3.200846 --> 3.175027).  Saving model ...\n",
            "402m 36s (- 603501m 17s) (50 0%) 4673.1381\n",
            " iter 50\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 51\n",
            "Validation loss decreased (3.175027 --> 3.162060).  Saving model ...\n",
            " iter 52\n",
            "Validation loss decreased (3.162060 --> 3.156524).  Saving model ...\n",
            " iter 53\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 54\n",
            "Validation loss decreased (3.156524 --> 3.150798).  Saving model ...\n",
            "442m 49s (- 603398m 48s) (55 0%) 4638.4205\n",
            " iter 55\n",
            "Validation loss decreased (3.150798 --> 3.142618).  Saving model ...\n",
            " iter 56\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 57\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 58\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 59\n",
            "EarlyStopping counter: 4 out of 20\n",
            "483m 0s (- 603286m 49s) (60 0%) 4605.8103\n",
            " iter 60\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 61\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 62\n",
            "EarlyStopping counter: 7 out of 20\n",
            " iter 63\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 64\n",
            "EarlyStopping counter: 9 out of 20\n",
            "523m 9s (- 603124m 1s) (65 0%) 4571.8777\n",
            " iter 65\n",
            "EarlyStopping counter: 10 out of 20\n",
            " iter 66\n",
            "EarlyStopping counter: 11 out of 20\n",
            " iter 67\n",
            "Validation loss decreased (3.142618 --> 3.108828).  Saving model ...\n",
            " iter 68\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 69\n",
            "EarlyStopping counter: 2 out of 20\n",
            "563m 21s (- 603039m 19s) (70 0%) 4553.0212\n",
            " iter 70\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 71\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 72\n",
            "Validation loss decreased (3.108828 --> 3.096931).  Saving model ...\n",
            " iter 73\n",
            "Validation loss decreased (3.096931 --> 3.064699).  Saving model ...\n",
            " iter 74\n",
            "EarlyStopping counter: 1 out of 20\n",
            "603m 33s (- 602949m 24s) (75 0%) 4525.1342\n",
            " iter 75\n",
            "Validation loss decreased (3.064699 --> 3.060434).  Saving model ...\n",
            " iter 76\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 77\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 78\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 79\n",
            "EarlyStopping counter: 4 out of 20\n",
            "643m 44s (- 602869m 34s) (80 0%) 4500.9227\n",
            " iter 80\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 81\n",
            "Validation loss decreased (3.060434 --> 3.037292).  Saving model ...\n",
            " iter 82\n",
            "Validation loss decreased (3.037292 --> 3.031079).  Saving model ...\n",
            " iter 83\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 84\n",
            "EarlyStopping counter: 2 out of 20\n",
            "683m 55s (- 602778m 45s) (85 0%) 4467.9798\n",
            " iter 85\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 86\n",
            "Validation loss decreased (3.031079 --> 3.021573).  Saving model ...\n",
            " iter 87\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 88\n",
            "Validation loss decreased (3.021573 --> 3.003816).  Saving model ...\n",
            " iter 89\n",
            "EarlyStopping counter: 1 out of 20\n",
            "724m 8s (- 602727m 24s) (90 0%) 4450.4842\n",
            " iter 90\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 91\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 92\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 93\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 94\n",
            "EarlyStopping counter: 6 out of 20\n",
            "764m 15s (- 602601m 17s) (95 0%) 4419.3437\n",
            " iter 95\n",
            "EarlyStopping counter: 7 out of 20\n",
            " iter 96\n",
            "Validation loss decreased (3.003816 --> 2.982664).  Saving model ...\n",
            " iter 97\n",
            "Validation loss decreased (2.982664 --> 2.969340).  Saving model ...\n",
            " iter 98\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 99\n",
            "EarlyStopping counter: 2 out of 20\n",
            "804m 25s (- 602520m 31s) (100 0%) 4393.8518\n",
            " iter 100\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 101\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 102\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 103\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 104\n",
            "EarlyStopping counter: 7 out of 20\n",
            "844m 35s (- 602429m 59s) (105 0%) 4373.8603\n",
            " iter 105\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 106\n",
            "Validation loss decreased (2.969340 --> 2.948718).  Saving model ...\n",
            " iter 107\n",
            "Validation loss decreased (2.948718 --> 2.941977).  Saving model ...\n",
            " iter 108\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 109\n",
            "EarlyStopping counter: 2 out of 20\n",
            "884m 45s (- 602359m 41s) (110 0%) 4351.6165\n",
            " iter 110\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 111\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 112\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 113\n",
            "Validation loss decreased (2.941977 --> 2.931688).  Saving model ...\n",
            " iter 114\n",
            "Validation loss decreased (2.931688 --> 2.925155).  Saving model ...\n",
            "924m 58s (- 602314m 41s) (115 0%) 4332.5844\n",
            " iter 115\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 116\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 117\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 118\n",
            "Validation loss decreased (2.925155 --> 2.909762).  Saving model ...\n",
            " iter 119\n",
            "EarlyStopping counter: 1 out of 20\n",
            "965m 8s (- 602246m 25s) (120 0%) 4307.4744\n",
            " iter 120\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 121\n",
            "Validation loss decreased (2.909762 --> 2.900301).  Saving model ...\n",
            " iter 122\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 123\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 124\n",
            "EarlyStopping counter: 3 out of 20\n",
            "1005m 17s (- 602166m 25s) (125 0%) 4288.9748\n",
            " iter 125\n",
            "Validation loss decreased (2.900301 --> 2.898283).  Saving model ...\n",
            " iter 126\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 127\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 128\n",
            "Validation loss decreased (2.898283 --> 2.890708).  Saving model ...\n",
            " iter 129\n",
            "EarlyStopping counter: 1 out of 20\n",
            "1045m 35s (- 602176m 30s) (130 0%) 4264.4756\n",
            " iter 130\n",
            "Validation loss decreased (2.890708 --> 2.883646).  Saving model ...\n",
            " iter 131\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 132\n",
            "Validation loss decreased (2.883646 --> 2.882298).  Saving model ...\n",
            " iter 133\n",
            "Validation loss decreased (2.882298 --> 2.871352).  Saving model ...\n",
            " iter 134\n",
            "EarlyStopping counter: 1 out of 20\n",
            "1085m 47s (- 602132m 3s) (135 0%) 4244.2145\n",
            " iter 135\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 136\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 137\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 138\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 139\n",
            "Validation loss decreased (2.871352 --> 2.866034).  Saving model ...\n",
            "1125m 57s (- 602066m 27s) (140 0%) 4221.0911\n",
            " iter 140\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 141\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 142\n",
            "Validation loss decreased (2.866034 --> 2.859853).  Saving model ...\n",
            " iter 143\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 144\n",
            "Validation loss decreased (2.859853 --> 2.855328).  Saving model ...\n",
            "1166m 9s (- 602023m 5s) (145 0%) 4200.6035\n",
            " iter 145\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 146\n",
            "Validation loss decreased (2.855328 --> 2.842356).  Saving model ...\n",
            " iter 147\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 148\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 149\n",
            "EarlyStopping counter: 3 out of 20\n",
            "1206m 22s (- 601978m 54s) (150 0%) 4176.7716\n",
            " iter 150\n",
            "Validation loss decreased (2.842356 --> 2.840997).  Saving model ...\n",
            " iter 151\n",
            "Validation loss decreased (2.840997 --> 2.835421).  Saving model ...\n",
            " iter 152\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 153\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 154\n",
            "EarlyStopping counter: 3 out of 20\n",
            "1246m 37s (- 601956m 4s) (155 0%) 4155.6350\n",
            " iter 155\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 156\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 157\n",
            "Validation loss decreased (2.835421 --> 2.818571).  Saving model ...\n",
            " iter 158\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 159\n",
            "EarlyStopping counter: 2 out of 20\n",
            "1286m 53s (- 601945m 45s) (160 0%) 4145.8147\n",
            " iter 160\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 161\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 162\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 163\n",
            "Validation loss decreased (2.818571 --> 2.816523).  Saving model ...\n",
            " iter 164\n",
            "EarlyStopping counter: 1 out of 20\n",
            "1327m 13s (- 601955m 30s) (165 0%) 4121.5898\n",
            " iter 165\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 166\n",
            "Validation loss decreased (2.816523 --> 2.810028).  Saving model ...\n",
            " iter 167\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 168\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 169\n",
            "EarlyStopping counter: 3 out of 20\n",
            "1367m 29s (- 601934m 45s) (170 0%) 4103.0708\n",
            " iter 170\n",
            "Validation loss decreased (2.810028 --> 2.808653).  Saving model ...\n",
            " iter 171\n",
            "Validation loss decreased (2.808653 --> 2.804434).  Saving model ...\n",
            " iter 172\n",
            "Validation loss decreased (2.804434 --> 2.801076).  Saving model ...\n",
            " iter 173\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 174\n",
            "EarlyStopping counter: 2 out of 20\n",
            "1407m 42s (- 601897m 33s) (175 0%) 4082.1040\n",
            " iter 175\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 176\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 177\n",
            "Validation loss decreased (2.801076 --> 2.783413).  Saving model ...\n",
            " iter 178\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 179\n",
            "EarlyStopping counter: 2 out of 20\n",
            "1447m 57s (- 601871m 4s) (180 0%) 4068.8504\n",
            " iter 180\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 181\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 182\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 183\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 184\n",
            "EarlyStopping counter: 7 out of 20\n",
            "1488m 12s (- 601842m 43s) (185 0%) 4049.1898\n",
            " iter 185\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 186\n",
            "EarlyStopping counter: 9 out of 20\n",
            " iter 187\n",
            "EarlyStopping counter: 10 out of 20\n",
            " iter 188\n",
            "Validation loss decreased (2.783413 --> 2.773406).  Saving model ...\n",
            " iter 189\n",
            "Validation loss decreased (2.773406 --> 2.772354).  Saving model ...\n",
            "1528m 31s (- 601839m 17s) (190 0%) 4032.8728\n",
            " iter 190\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 191\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 192\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 193\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 194\n",
            "EarlyStopping counter: 5 out of 20\n",
            "1568m 48s (- 601818m 7s) (195 0%) 4010.5051\n",
            " iter 195\n",
            "Validation loss decreased (2.772354 --> 2.768970).  Saving model ...\n",
            " iter 196\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 197\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 198\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 199\n",
            "Validation loss decreased (2.768970 --> 2.756486).  Saving model ...\n",
            "1609m 6s (- 601806m 6s) (200 0%) 3992.1102\n",
            " iter 200\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 201\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 202\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 203\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 204\n",
            "Validation loss decreased (2.756486 --> 2.746532).  Saving model ...\n",
            "1649m 21s (- 601772m 47s) (205 0%) 3970.7232\n",
            " iter 205\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 206\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 207\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 208\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 209\n",
            "Validation loss decreased (2.746532 --> 2.743398).  Saving model ...\n",
            "1689m 36s (- 601740m 55s) (210 0%) 3960.2080\n",
            " iter 210\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 211\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 212\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 213\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 214\n",
            "EarlyStopping counter: 5 out of 20\n",
            "1729m 51s (- 601708m 39s) (215 0%) 3943.0628\n",
            " iter 215\n",
            "Validation loss decreased (2.743398 --> 2.739984).  Saving model ...\n",
            " iter 216\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 217\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 218\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 219\n",
            "Validation loss decreased (2.739984 --> 2.731272).  Saving model ...\n",
            "1770m 6s (- 601678m 40s) (220 0%) 3922.0418\n",
            " iter 220\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 221\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 222\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 223\n",
            "Validation loss decreased (2.731272 --> 2.719084).  Saving model ...\n",
            " iter 224\n",
            "EarlyStopping counter: 1 out of 20\n",
            "1810m 23s (- 601656m 2s) (225 0%) 3907.9541\n",
            " iter 225\n",
            "Validation loss decreased (2.719084 --> 2.711081).  Saving model ...\n",
            " iter 226\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 227\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 228\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 229\n",
            "EarlyStopping counter: 4 out of 20\n",
            "1850m 40s (- 601629m 45s) (230 0%) 3892.5068\n",
            " iter 230\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 231\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 232\n",
            "EarlyStopping counter: 7 out of 20\n",
            " iter 233\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 234\n",
            "EarlyStopping counter: 9 out of 20\n",
            "1890m 54s (- 601590m 53s) (235 0%) 3870.3560\n",
            " iter 235\n",
            "EarlyStopping counter: 10 out of 20\n",
            " iter 236\n",
            "Validation loss decreased (2.711081 --> 2.706773).  Saving model ...\n",
            " iter 237\n",
            "Validation loss decreased (2.706773 --> 2.702026).  Saving model ...\n",
            " iter 238\n",
            "Validation loss decreased (2.702026 --> 2.698853).  Saving model ...\n",
            " iter 239\n",
            "EarlyStopping counter: 1 out of 20\n",
            "1931m 21s (- 601617m 19s) (240 0%) 3859.6085\n",
            " iter 240\n",
            "Validation loss decreased (2.698853 --> 2.697344).  Saving model ...\n",
            " iter 241\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 242\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 243\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 244\n",
            "EarlyStopping counter: 4 out of 20\n",
            "1971m 45s (- 601630m 8s) (245 0%) 3839.8905\n",
            " iter 245\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 246\n",
            "Validation loss decreased (2.697344 --> 2.692689).  Saving model ...\n",
            " iter 247\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 248\n",
            "Validation loss decreased (2.692689 --> 2.686531).  Saving model ...\n",
            " iter 249\n",
            "Validation loss decreased (2.686531 --> 2.679020).  Saving model ...\n",
            "2012m 8s (- 601631m 11s) (250 0%) 3822.0402\n",
            " iter 250\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 251\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 252\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 253\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 254\n",
            "EarlyStopping counter: 5 out of 20\n",
            "2052m 38s (- 601663m 14s) (255 0%) 3812.5491\n",
            " iter 255\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 256\n",
            "EarlyStopping counter: 7 out of 20\n",
            " iter 257\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 258\n",
            "EarlyStopping counter: 9 out of 20\n",
            " iter 259\n",
            "EarlyStopping counter: 10 out of 20\n",
            "2093m 3s (- 601673m 45s) (260 0%) 3797.2900\n",
            " iter 260\n",
            "EarlyStopping counter: 11 out of 20\n",
            " iter 261\n",
            "EarlyStopping counter: 12 out of 20\n",
            " iter 262\n",
            "Validation loss decreased (2.679020 --> 2.674952).  Saving model ...\n",
            " iter 263\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 264\n",
            "EarlyStopping counter: 2 out of 20\n",
            "2133m 55s (- 601805m 6s) (265 0%) 3771.7309\n",
            " iter 265\n",
            "Validation loss decreased (2.674952 --> 2.673806).  Saving model ...\n",
            " iter 266\n",
            "Validation loss decreased (2.673806 --> 2.671866).  Saving model ...\n",
            " iter 267\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 268\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 269\n",
            "EarlyStopping counter: 3 out of 20\n",
            "2197m 25s (- 608199m 43s) (270 0%) 3767.0707\n",
            " iter 270\n",
            "Validation loss decreased (2.671866 --> 2.669547).  Saving model ...\n",
            " iter 271\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 272\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 273\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 274\n",
            "EarlyStopping counter: 4 out of 20\n",
            "2590m 24s (- 703884m 56s) (275 0%) 3747.7287\n",
            " iter 275\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 276\n",
            "Validation loss decreased (2.669547 --> 2.668224).  Saving model ...\n",
            " iter 277\n",
            "Validation loss decreased (2.668224 --> 2.660314).  Saving model ...\n",
            " iter 278\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 279\n",
            "EarlyStopping counter: 2 out of 20\n",
            "2958m 3s (- 789380m 34s) (280 0%) 3725.7184\n",
            " iter 280\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 281\n",
            "EarlyStopping counter: 4 out of 20\n",
            " iter 282\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 283\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 284\n",
            "EarlyStopping counter: 7 out of 20\n",
            "3116m 3s (- 816901m 26s) (285 0%) 3715.0256\n",
            " iter 285\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 286\n",
            "EarlyStopping counter: 9 out of 20\n",
            " iter 287\n",
            "EarlyStopping counter: 10 out of 20\n",
            " iter 288\n",
            "EarlyStopping counter: 11 out of 20\n",
            " iter 289\n",
            "EarlyStopping counter: 12 out of 20\n",
            "3206m 34s (- 826080m 29s) (290 0%) 3701.7687\n",
            " iter 290\n",
            "Validation loss decreased (2.660314 --> 2.657258).  Saving model ...\n",
            " iter 291\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 292\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 293\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 294\n",
            "EarlyStopping counter: 4 out of 20\n",
            "3297m 2s (- 834932m 13s) (295 0%) 3692.6764\n",
            " iter 295\n",
            "Validation loss decreased (2.657258 --> 2.656828).  Saving model ...\n",
            " iter 296\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 297\n",
            "EarlyStopping counter: 2 out of 20\n",
            " iter 298\n",
            "EarlyStopping counter: 3 out of 20\n",
            " iter 299\n",
            "EarlyStopping counter: 4 out of 20\n",
            "3387m 23s (- 843460m 57s) (300 0%) 3676.5546\n",
            " iter 300\n",
            "EarlyStopping counter: 5 out of 20\n",
            " iter 301\n",
            "EarlyStopping counter: 6 out of 20\n",
            " iter 302\n",
            "EarlyStopping counter: 7 out of 20\n",
            " iter 303\n",
            "EarlyStopping counter: 8 out of 20\n",
            " iter 304\n",
            "EarlyStopping counter: 9 out of 20\n",
            "3478m 22s (- 851861m 4s) (305 0%) 3669.2779\n",
            " iter 305\n",
            "EarlyStopping counter: 10 out of 20\n",
            " iter 306\n",
            "Validation loss decreased (2.656828 --> 2.652361).  Saving model ...\n",
            " iter 307\n",
            "EarlyStopping counter: 1 out of 20\n",
            " iter 308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBv1RrOGkFaY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCtE2frk4Gt8"
      },
      "source": [
        "#Time 00:32  pm 27-10-2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm48_lWkFaY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI_FUg_yd80z"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden(batch_size=1)\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei,:] += encoder_output[:, 0 , :].squeeze()\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        decoder_input = decoder_input.view(1,1)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        encoder_outputs = encoder_outputs.view(1 , encoder_outputs.shape[0] , encoder_outputs.shape[1])\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "            decoder_input = decoder_input.view(1,-1)\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PDwQY2jXnU0"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksrSnzFxS_8P"
      },
      "source": [
        "# #Saving should be as simple as: \n",
        "# torch.save(encoder1.state_dict(), '/content/encoder.dict')\n",
        "# torch.save(attn_decoder1.state_dict(), '/content/decoder.dict')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDbp6l1St8qC"
      },
      "source": [
        "s1 = \"it is found in the region pays de la loire in the sarthe department in the west of france .\"\n",
        "output_words, attentions = evaluate(encoder1, attn_decoder1, s1)\n",
        "output_sentence = ' '.join(output_words)\n",
        "print('<', output_sentence)\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEl6iHWbkFaY"
      },
      "source": [
        "s2 = \"the ahom kings reigned for close to years .\"\n",
        "output_words, attentions = evaluate(encoder1, attn_decoder1, s2)\n",
        "output_sentence = ' '.join(output_words)\n",
        "print('<', output_sentence)\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrTuvyTTkFaY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRvG6yWwkFaY"
      },
      "source": [
        "\"\"\"\n",
        "# writing to file\n",
        "MyFileSimple=open('./TestData/simple_test','w')\n",
        "for ss, cs in test_loader:    \n",
        "    MyFileSimple.write(str(ss))\n",
        "    MyFileSimple.write('\\n')\n",
        "MyFileSimple.close()\n",
        "# complex\n",
        "MyFileComplex=open('./TestData/complex_test','w')\n",
        "for ss,cs in test_loader:    \n",
        "    MyFileComplex.write(str(cs))\n",
        "    MyFileComplex.write('\\n')\n",
        "MyFileComplex.close()\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}